# ü§ñ LLM Integration - –¢–µ—Ö–Ω–∏—á–µ—Å–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ

## 1. üèóÔ∏è –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ LLM

### –ò—Å–ø–æ–ª—å–∑—É–µ–º–∞—è –º–æ–¥–µ–ª—å
```
Provider: OpenRouter
Model: anthropic/claude-3-haiku
Version: Latest
Cost: ~$0.80 per million input tokens, ~$4 per million output tokens
Context: 200K tokens
Temperature: 0.5 (balanced)
Max tokens: 1000‚Äì2000
```

### –ì–¥–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è LLM

```
Backend (FastAPI)
‚îÇ
‚îú‚îÄ LLMAnalyzer (backend/utils/llm_analyzer.py)
‚îÇ  ‚îú‚îÄ analyze_client_sentiment()           ‚Üê Client insights extraction
‚îÇ  ‚îú‚îÄ check_checklist_item_semantic()      ‚Üê Checklist validation
‚îÇ  ‚îú‚îÄ identify_speakers()                  ‚Üê Speaker diarization (optional)
‚îÇ  ‚îî‚îÄ generate_next_step()                 ‚Üê Next step recommendation (optional)
‚îÇ
‚îî‚îÄ IntentDetector (backend/utils/intent_detector.py)
   ‚îî‚îÄ detect_trigger()                     ‚Üê Keyword/regex only (NO LLM)
```

---

## 2. üìç LLMAnalyzer - –ì–¥–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è

### 2.1 `analyze_client_sentiment()`

**–í—ã–∑—ã–≤–∞–µ—Ç—Å—è:** –ö–∞–∂–¥—ã–µ 5 —Å–µ–∫—É–Ω–¥ –ø—Ä–∏ –Ω–∞–ª–∏—á–∏–∏ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞

**–ö–æ–≥–¥–∞ –≤—ã–∑—ã–≤–∞–µ—Ç—Å—è:**
- –í `/ws/ingest` —Ü–∏–∫–ª–µ (real-time live recording)
- –í `/api/process-transcript` (debug text mode)
- –í `/api/process-youtube` (YouTube mode)
- –í `/api/process-video` (video file mode)

**–í—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ:**
```python
client_text = "I'm hesitant. It's too expensive. But the game-based learning sounds fun."
full_transcript_context = "... previous 500 chars of transcript ..."
```

**Prompt:**
```
You are analyzing a sales call to understand the client's interests, objections, needs, and emotional state.

Client speech segment:
 I'm hesitant. It's too expensive. But the game-based learning sounds fun.
 
Full conversation context (last messages):
... context ...

Analyze and provide:
1. EMOTION: How is the client feeling? (engaged, curious, hesitant, defensive, negative, neutral)
2. INTERESTS: What topics interest them? (e.g., "game-based learning", "future skills", "logic", "creativity", "confidence")
3. OBJECTIONS: What are their concerns or obstacles? (e.g., "price", "time", "family", "value", "feasibility")
4. NEEDS: What is their core need or pain point?
5. ENGAGEMENT_LEVEL: 0.0-1.0 scale of how engaged they seem
6. STAGE_HINT: What stage of the call is this? (greeting, profiling, presentation, objection, closing)

CONFIDENCE: Only extract interests/objections if you are confident they were explicitly mentioned.
Avoid false positives from unclear or partial utterances.

Return ONLY valid JSON with no extra text:
{
  "emotion": "engaged|curious|hesitant|defensive|negative|neutral",
  "interests": ["topic1", "topic2"],
  "objections": ["concern1", "concern2"],
  "needs": "core need or pain point",
  "engagement_level": 0.75,
  "stage_hint": "profiling|presentation|objection|closing",
  "buying_signals": ["signal1", "signal2"],
  "reasoning": "Brief explanation of the analysis"
}

Focus on MEANING and CONTEXT, not just keywords. Understand what the client truly cares about.
```

**–í—ã—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ:**
```json
{
  "emotion": "hesitant",
  "interests": ["game-based learning"],
  "objections": ["price"],
  "needs": "Affordable solution",
  "engagement_level": 0.7,
  "stage_hint": "objection",
  "buying_signals": [],
  "reasoning": "Client hesitant about price but interested in game-based learning..."
}
```

**Guard clauses:**
```python
# Skip if text too short
if len(client_text.strip()) < 20:
    return {
        "emotion": "neutral",
        "objections": [],
        "interests": [],
        "needs": None,
        "engagement_level": 0.3,
        "stage_hint": "profiling",
        "buying_signals": [],
        "reasoning": "Text too short for analysis"
    }
```

**–ì–¥–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç:**
```
JSON ‚Üí Backend state
 ‚îî‚îÄ last_client_insight (cache)
    ‚îî‚îÄ WebSocket /coach broadcast
       ‚îî‚îÄ Frontend: ClientInfoSummary component
```

---

### 2.2 `check_checklist_item_semantic()`

**–í—ã–∑—ã–≤–∞–µ—Ç—Å—è:** –ö–∞–∂–¥—ã–µ 5 —Å–µ–∫—É–Ω–¥, –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –Ω–µ–∑–∞–≤–µ—Ä—à—ë–Ω–Ω–æ–≥–æ –ø—É–Ω–∫—Ç–∞ —á–µ–∫–ª–∏—Å—Ç–∞

**–ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ (3 —É—Ä–æ–≤–Ω—è):**
```
Level 1: Permanent completion
  if item.completed == True:
    ‚îî‚îÄ SKIP (never re-check)

Level 2: 30-second cooldown
  if item.id in checklist_completion_cache:
    if time.now() - cache[item.id] < 30s:
      ‚îî‚îÄ SKIP

Level 3: 60-second LLM cache
  if item.id in checklist_llm_cache:
    if time.now() - cache[item.id] < 60s:
      ‚îî‚îÄ Reuse cached response
```

**–í—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ:**
```python
item_text = "Introduce yourself and company"
conversation_context = "Hi, I'm John. We help kids learn coding..."
language = "en"
```

**Prompt:**
```
You are a sales call analyzer. Determine if a specific sales action has been completed in the conversation.

Sales action to check:
"Introduce yourself and company"

Conversation so far (last 2000 chars):
Hi, I'm John. We help kids learn coding...

Task:
1. Determine if this action was clearly done in the conversation
2. Provide confidence level (0.0-1.0)
3. Extract the exact text evidence (2 sentences max)

Return ONLY valid JSON:
{
  "completed": true|false,
  "confidence": 0.95,
  "evidence": "Exact text from conversation proving completion"
}

STRICT RULES:
- Confidence must be >= 0.8 to mark as completed
- Evidence must be from actual conversation text
- If unsure, return confidence < 0.8
```

**–í—ã—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ:**
```json
{
  "completed": true,
  "confidence": 0.95,
  "evidence": "Hi, I'm John. We help kids learn coding."
}
```

**Validation logic:**
```python
if response["completed"] and response.get("confidence", 0) >= 0.8:
    checklist_progress[item_id] = {"completed": True}
    checklist_evidence[item_id] = response["evidence"]
    checklist_completion_cache[item_id] = time.time()
else:
    # Don't mark complete, retry next cycle (with cooldown)
    pass
```

**Guard clauses:**
```python
# Skip if text too short
if len(conversation_context) < 30:
    return False, ""

# Skip if LLM confidence too low
if llm_response.get("confidence", 0) < 0.8:
    return False, ""
```

**–ì–¥–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç:**
```
JSON ‚Üí Backend state
 ‚îî‚îÄ checklist_progress (Dict[item_id, {completed: bool}])
 ‚îî‚îÄ checklist_evidence (Dict[item_id, evidence_text])
    ‚îî‚îÄ WebSocket /coach broadcast
       ‚îî‚îÄ Frontend: CallChecklist component
```

---

### 2.3 `identify_speakers()` (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)

**–í—ã–∑—ã–≤–∞–µ—Ç—Å—è:** –û–¥–∏–Ω —Ä–∞–∑ –ø—Ä–∏ –Ω–∞–ª–∏—á–∏–∏ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞

**–í—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ:**
```python
transcript = "Hi, I'm John. That's great. What about pricing? It's 100 dollars."
```

**Prompt:**
```
Identify who is speaking in this sales call transcript. Mark each sentence with [SALES] or [CLIENT].

Format:
[SALES] Hi, I'm John.
[CLIENT] That's great.
[SALES] What about pricing?
[CLIENT] It's 100 dollars.

Return ONLY the formatted transcript with no extra text.
```

**–í—ã—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ:**
```python
[
    {"speaker": "sales", "text": "Hi, I'm John."},
    {"speaker": "client", "text": "That's great."},
    {"speaker": "sales", "text": "What about pricing?"},
    {"speaker": "client", "text": "It's 100 dollars."}
]
```

**–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:**
```
Extract client_text from identified [CLIENT] segments
‚îî‚îÄ Pass to analyze_client_sentiment()
```

---

### 2.4 `generate_next_step()` (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)

**–í—ã–∑—ã–≤–∞–µ—Ç—Å—è:** –û–¥–∏–Ω —Ä–∞–∑ –∫–∞–∂–¥—ã–µ 15-30 —Å–µ–∫—É–Ω–¥

**–í—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ:**
```python
current_stage = "objection"
client_insight = {"emotion": "hesitant", "objections": ["price"], ...}
checklist_progress = {"intro_yourself": {"completed": True}, ...}
conversation_context = "..."
```

**Prompt:**
```
Based on the sales call state, provide the next recommended action for the sales manager.

Current stage: objection
Client state:
- Emotion: hesitant
- Objections: price
- Interests: game-based learning

Completed actions:
- Introduced yourself and company
- Identified pain points

Conversation so far: ...

Provide ONE concise, actionable next step (max 2 sentences) that:
1. Addresses the current objection
2. Follows best sales practices
3. Is specific and contextual

Return plain text (no JSON).
```

**–í—ã—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ:**
```
"Address the price objection: Show ROI calculations or offer flexible payment options."
```

**–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:**
```
next_step = response
‚îî‚îÄ WebSocket /coach: "next_step" field
   ‚îî‚îÄ Frontend: NextStepCard component
```

---

## 3. üîß API Integration - OpenRouter

### Request format

```python
import requests
import json

url = "https://openrouter.ai/api/v1/chat/completions"

headers = {
    "Authorization": f"Bearer {OPENROUTER_API_KEY}",
    "Content-Type": "application/json"
}

payload = {
    "model": "anthropic/claude-3-haiku",
    "messages": [
        {
            "role": "system",
            "content": "You are a sales coach analyzing client calls..."
        },
        {
            "role": "user",
            "content": "Analyze this client speech: ..."
        }
    ],
    "temperature": 0.5,
    "max_tokens": 1000,
    "response_format": {"type": "json_object"}  # For JSON mode
}

response = requests.post(url, headers=headers, json=payload, timeout=10)
result = response.json()
llm_response_text = result["choices"][0]["message"]["content"]
```

### Error handling

```python
try:
    response = requests.post(url, headers=headers, json=payload, timeout=10)
    response.raise_for_status()
    result = response.json()
    
    if "error" in result:
        raise Exception(f"OpenRouter error: {result['error']}")
    
    return result["choices"][0]["message"]["content"]
    
except requests.Timeout:
    print("‚ö†Ô∏è LLM timeout (10s)")
    return None
except requests.RequestException as e:
    print(f"‚ö†Ô∏è LLM request failed: {e}")
    return None
except json.JSONDecodeError:
    print("‚ö†Ô∏è LLM response not valid JSON")
    return None
except Exception as e:
    print(f"‚ö†Ô∏è LLM error: {e}")
    return None
```

### Fallback strategy

```python
# If LLM fails ‚Üí Use keyword-based analysis
try:
    llm_result = llm_analyzer.analyze_client_sentiment(client_text, context)
except Exception as e:
    print(f"‚ö†Ô∏è LLM analysis failed: {e}, using fallback")
    llm_result = keyword_analyzer.analyze_client_text(client_text)
    # Returns: {emotion, objections, interests, needs, engagement, stage}
```

---

## 4. üìä Cost estimation

### Per 5-second update cycle

```
Assumptions:
- client_text: ~100 tokens (average)
- full_context: ~200 tokens
- Per request: ~300 tokens input

Per cycle (5s):
  1. analyze_client_sentiment()
     ‚îî‚îÄ 1 LLM call √ó 300 tokens ‚âà $0.00024 (input only)

Per cycle per checklist item (with cooldown):
  2. check_checklist_item_semantic()
     ‚îî‚îÄ ~1 call per 30s (with 30s cooldown)
     ‚îî‚îÄ Per cycle: 1/6 call √ó 300 tokens ‚âà $0.00004

Total per minute (12 cycles):
  - analyze_client_sentiment: 12 √ó $0.00024 ‚âà $0.00288
  - check_checklist_item: (varies) ‚âà $0.0005‚Äì0.001
  - Total: ~$0.003‚Äì0.005 per minute
  - Per hour: ~$0.18‚Äì0.30
  - Per day: ~$4‚Äì7
```

### Optimization techniques already in place

```
1. 30-second cooldown for checklist items
   ‚îî‚îÄ Reduces calls by 83% per item

2. 60-second LLM response cache
   ‚îî‚îÄ Reuses response if identical request within 60s

3. Guard clauses (skip short texts)
   ‚îî‚îÄ Text < 20 chars ‚Üí skip analyze_client_sentiment
   ‚îî‚îÄ Text < 30 chars ‚Üí skip check_checklist_item

4. Permanent completion cache
   ‚îî‚îÄ Once item.completed = True ‚Üí never re-check

5. Keyword-based fallback
   ‚îî‚îÄ If LLM fails ‚Üí use fast keyword matching
```

---

## 5. üéØ Where IntentDetector does NOT use LLM

### IntentDetector (backend/utils/intent_detector.py)

```python
def detect_trigger(transcript: str) -> Dict | None:
    """
    Detect sales triggers using KEYWORD MATCHING ONLY (no LLM)
    """
    
    # Step 1: Keyword matching against playbook
    for trigger in self.playbook:
        for keyword in trigger["match"]:
            if keyword.lower() in transcript.lower():
                # Step 2: Priority selection
                if not self.active_trigger or trigger["priority"] > self.active_trigger_priority:
                    # Step 3: Anti-spam cooldown (30s)
                    if time.time() - self.last_trigger_time > 30:
                        self.active_trigger = trigger
                        self.last_trigger_time = time.time()
                        return trigger
    
    return None
```

**Why no LLM here?**
- Real-time requirement (instant response needed)
- Keyword matching is fast enough (regex only)
- Cost savings
- 25 triggers in playbook

**Performance:** ~1ms per transcript

---

## 6. üîÑ Request/Response lifecycle

### Example: analyze_client_sentiment flow

```
‚îå‚îÄ Client speaks: "It's too expensive"
‚îÇ
‚îú‚îÄ /ws/ingest receives audio chunk (PCM)
‚îÇ
‚îú‚îÄ AudioBuffer accumulates for 5s
‚îÇ
‚îú‚îÄ faster-whisper transcribes
‚îÇ  ‚îî‚îÄ transcript = "It's too expensive"
‚îÇ
‚îú‚îÄ LLMAnalyzer.identify_speakers()
‚îÇ  ‚îî‚îÄ [CLIENT] It's too expensive
‚îÇ
‚îú‚îÄ LLMAnalyzer.analyze_client_sentiment()
‚îÇ  ‚îî‚îÄ Build prompt + send to OpenRouter
‚îÇ  ‚îî‚îÄ Wait for response (timeout: 10s)
‚îÇ  ‚îî‚îÄ Parse JSON
‚îÇ     {
‚îÇ       "emotion": "negative",
‚îÇ       "objections": ["price"],
‚îÇ       "interests": [],
‚îÇ       "needs": "Lower price",
‚îÇ       "engagement_level": 0.4,
‚îÇ       "stage_hint": "objection"
‚îÇ     }
‚îÇ
‚îú‚îÄ IntentDetector.detect_trigger()
‚îÇ  ‚îî‚îÄ Regex match: "expensive" in "price_objection"
‚îÇ  ‚îî‚îÄ Return: {id: "price_objection", title: "üí∞ Client...", hint: "..."}
‚îÇ
‚îú‚îÄ Send WebSocket message:
‚îÇ  {
‚îÇ    "client_insight": {...},
‚îÇ    "assist_trigger": {...},
‚îÇ    "hint": "...",
‚îÇ    "prob": 0.8
‚îÇ  }
‚îÇ
‚îî‚îÄ Frontend updates UI (throttled 1/sec)
```

---

## 7. üõë Known limitations

### Rate limiting (implicit)

```
No explicit rate limiting, but:
- 5-second update cycle = 12 LLM calls/minute max
- OpenRouter default: generous (no strict limits mentioned)
- Practical limit: ~1000 calls/day before hitting cost concerns
```

### Timeout handling

```
If LLM response > 10 seconds:
  ‚îî‚îÄ Request aborts
  ‚îî‚îÄ Fallback to keyword analysis
  ‚îî‚îÄ User sees delayed recommendation
```

### Context size

```
Max input per request: ~1500 tokens
- client_text: ~100 tokens
- full_context: ~200 tokens
- Prompt template: ~1000 tokens
Total: ~1300 tokens (safe margin)
```

### Temperature sensitivity

```
Current: temperature=0.5 (balanced)
- Lower (0.2): More deterministic, consistent
- Higher (0.9): More creative, variable
For checklist: Consider lowering to 0.3 for stricter validation
```

---

## 8. üìà Monitoring

### Metrics to track

```python
class SystemStatus:
    llm_analysis_count: int          # Total LLM calls made
    llm_analysis_errors: int         # Failed LLM calls
    llm_cache_hits: int              # Responses from cache
    avg_llm_latency_ms: float        # Average response time
    checklist_validations: int       # Total checklist checks
    checklist_false_positives: int   # Incorrect markings
    
    # Per endpoint
    process_transcript_llm_calls: int
    process_youtube_llm_calls: int
    process_video_llm_calls: int
```

### Log examples

```
ü§ñ LLM Analyzer initialized with model: anthropic/claude-3-haiku
üîç ANALYZING CLIENT TEXT:
   üìù Input (66 chars): –Ø –Ω–µ —É–≤–µ—Ä–µ–Ω —á—Ç–æ —ç—Ç–æ—Ç –∫—É—Ä—Å –ø–æ–¥—Ö–æ–¥–∏—Ç...
   ‚úÖ LLM Analysis:
      Emotion: hesitant
      Interests: ['future skills', 'value']
      Objections: ['feasibility']
      Engagement: 0.7

‚è≠Ô∏è Text too short (13 chars), using minimal analysis
   (guard clause triggered)

‚ö†Ô∏è Sentiment analysis failed: timeout
   (fallback to keyword analysis)
```

---

## 9. üîê Security

### API Key management

```python
# .env file (never commit)
OPENROUTER_API_KEY=sk-or-v1-...

# Load in backend
from dotenv import load_dotenv
import os

load_dotenv()
OPENROUTER_API_KEY = os.getenv("OPENROUTER_API_KEY")

# Use in requests
headers = {"Authorization": f"Bearer {OPENROUTER_API_KEY}"}
```

### No sensitive data in prompts

```
‚ùå DON'T include:
- Client real names
- Phone numbers
- Email addresses
- Company names

‚úÖ DO include:
- Generic descriptions: "client", "child", "course"
- Anonymized speech: "The client said..."
```
