# ğŸ¯ Sales Best Friend - ĞÑ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ°

## 1. ğŸ”§ Ğ§Ñ‚Ğ¾ Ñ‚ĞµÑ…Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¿Ğ¾Ğ´ ĞºĞ°Ğ¿Ğ¾Ñ‚Ğ¾Ğ¼

### Stack

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ FRONTEND (React/Vite/TypeScript)    â”‚ BACKEND (FastAPI/Python)        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                     â”‚                                 â”‚
â”‚  Web Audio API                      â”‚  AudioBuffer                    â”‚
â”‚  (16kHz mono Int16 PCM)             â”‚  (accumulate 5s of data)        â”‚
â”‚         â†“                           â”‚         â†“                       â”‚
â”‚  WebSocket /ingest                  â”‚  faster-whisper                 â”‚
â”‚  (send PCM chunks)                  â”‚  (speech-to-text, multi-lang)   â”‚
â”‚                                     â”‚         â†“                       â”‚
â”‚  WebSocket /coach â†â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  (receive JSON updates)             â”‚                                 â”‚
â”‚         â†“                           â”‚  Parallel Processing:           â”‚
â”‚  React components                   â”‚  â”œâ”€ LLMAnalyzer                 â”‚
â”‚  - InCallAssist card                â”‚  â”‚  (Claude 3 Haiku via         â”‚
â”‚  - ClientInfoSummary                â”‚  â”‚   OpenRouter API)            â”‚
â”‚  - CallChecklist                    â”‚  â”‚                              â”‚
â”‚                                     â”‚  â”œâ”€ IntentDetector              â”‚
â”‚                                     â”‚  â”‚  (playbook.json matching)    â”‚
â”‚                                     â”‚  â”‚                              â”‚
â”‚                                     â”‚  â””â”€ Checklist validator         â”‚
â”‚                                     â”‚     (LLM-based semantic check)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Ğ¦Ğ¸ĞºĞ» Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ (ĞºĞ°Ğ¶Ğ´Ñ‹Ğµ 5 ÑĞµĞº)

```
1. Frontend: Audio buffer accumulates PCM chunks (Web Audio API)
   â””â”€ 8KB chunks from ScriptProcessorNode

2. Backend: AudioBuffer triggers when ready
   â””â”€ 163KB buffer = 5 sec of audio at 16kHz

3. Transcription: faster-whisper converts PCM â†’ text
   â””â”€ Language: configurable (en, id, ru, etc.)

4. Parallel LLM processing:
   â”œâ”€ LLMAnalyzer.analyze_client_sentiment()
   â”‚  â””â”€ Extract: emotion, objections, interests, needs, stage
   â”œâ”€ IntentDetector.detect_trigger()
   â”‚  â””â”€ Match keywords against playbook (25 triggers)
   â””â”€ Checklist validator
      â””â”€ LLMAnalyzer.check_checklist_item_semantic()

5. Send JSON via /coach WebSocket to all connected clients
   â””â”€ Rate-limited to 1 update/sec on frontend

6. Frontend: React re-renders components
   â””â”€ InCallAssist card (if trigger), ClientInfoSummary, CallChecklist
```

---

## 2. ğŸ“ In-Call Assist (ĞºĞ°Ñ€Ñ‚Ğ¾Ñ‡ĞºĞ° Ñ Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·ĞºĞ°Ğ¼Ğ¸)

### Flow

```
User speaks: "It's too expensive"
        â†“
Transcript received in backend
        â†“
IntentDetector.detect_trigger(transcript)
        â†“
Keyword matching against playbook:
  - Text: "it's too expensive"
  - Check each trigger in playbook
  - Match: "expensive" âˆˆ price_objection.match[]
        â†“
Priority-based selection:
  - price_objection: priority=10 (highest wins)
        â†“
Anti-spam cooldown (30s):
  - Skip if same trigger active
  - Skip if last trigger < 30s ago
        â†“
Send via WebSocket:
{
  "assist_trigger": {
    "id": "price_objection",
    "title": "ğŸ’° Client says it's too expensive",
    "hint": "Emphasize value, not price. Share success stories and offer a free intro lesson.",
    "priority": 10
  }
}
        â†“
Frontend: InCallAssist component
  - Fade in
  - Display for 10 seconds
  - Auto-dismiss (or manual close)
  - Only one card active at a time
```

### Playbook ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ° (playbook.json)

```json
[
  {
    "id": "price_objection",
    "match": ["Ğ´Ğ¾Ñ€Ğ¾Ğ³Ğ¾", "Ñ†ĞµĞ½Ğ°", "expensive", "costly", "mahal", "harga"],
    "title": "ğŸ’° Client says it's too expensive",
    "hint": "Emphasize value, not price. Share success stories and offer a free intro lesson.",
    "priority": 10
  },
  ...
]
```

**Ğ›Ğ¾Ğ³Ğ¸ĞºĞ°:** keyword regex matching â†’ priority selection â†’ anti-spam cooldown (30s) â†’ one active card

---

## 3. ğŸ‘¤ Key Client Information

### Flow

```
Transcript: "I'm hesitant. It's too expensive. But the game-based learning sounds fun."
        â†“
LLMAnalyzer.analyze_client_sentiment(client_text, full_context)
        â†“
Claude prompt:
  - Extract emotion: engaged|curious|hesitant|defensive|negative|neutral
  - Extract objections: [list of concerns]
  - Extract interests: [list of topics they like]
  - Extract needs: [core pain point]
  - Extract engagement_level: 0.0â€“1.0
  - Extract stage_hint: greeting|profiling|presentation|objection|closing
        â†“
Parse JSON response:
{
  "emotion": "hesitant",
  "objections": ["price"],
  "interests": ["game-based learning"],
  "needs": "Affordable solution that engages child",
  "engagement_level": 0.7,
  "stage_hint": "objection",
  "buying_signals": [],
  "reasoning": "Client is hesitant but interested in game-based learning..."
}
        â†“
Send via /coach WebSocket:
{
  "client_insight": {...}
}
        â†“
Frontend: ClientInfoSummary component
  - Display objections (red)
  - Display interests (blue)
  - Display needs (yellow)
  - Display emotion (emoji)
  - Update in real-time
```

### Guard clauses (Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸)

```python
# If text too short â†’ return neutral analysis (skip processing)
if len(client_text.strip()) < 20:
    return {emotion: "neutral", objections: [], ...}

# If LLM confidence too low â†’ skip item
if llm_confidence < 0.8:
    return False (don't mark checklist item)
```

**Ğ›Ğ¾Ğ³Ğ¸ĞºĞ°:** LLM semantic analysis â†’ JSON parsing â†’ cache results â†’ broadcast via WebSocket

---

## 4. ğŸ“‹ Call Progress Checklist

### Flow

```
Accumulated transcript: "Hi, I'm John. We help kids learn coding..."
        â†“
For each uncompleted checklist item in the active stage:
        â”œâ”€ if item.completed == True:
        â”‚  â””â”€ SKIP (permanent, never re-check)
        â”‚
        â”œâ”€ if item.id in checklist_completion_cache:
        â”‚  â””â”€ if last_check_time < 30s ago:
        â”‚     â””â”€ SKIP (cooldown)
        â”‚
        â””â”€ else: Call LLMAnalyzer.check_checklist_item_semantic()
        â†“
Claude prompt:
  "Is this sales action done?
   Item: 'Introduce yourself and company'
   Conversation: [last 2000 chars of transcript]
   Your answer: {completed: true/false, confidence: 0.0-1.0, evidence: '...'}"
        â†“
Parse response:
{
  "completed": true,
  "confidence": 0.95,
  "evidence": "Hi, I'm John from SalesBestFriend. We help kids learn coding..."
}
        â†“
Validation:
  - If confidence < 0.8 â†’ reject
  - If completed + confidence >= threshold:
    â””â”€ Mark item complete
    â””â”€ Store evidence (last 2 sentences)
    â””â”€ Store in checklist_evidence cache
    â””â”€ Update checklist_completion_cache
        â†“
Send via /coach WebSocket:
{
  "checklist_progress": {
    "greeting": {
      "intro_yourself": {
        "completed": true,
        "evidence": "Hi, I'm John from SalesBestFriend..."
      }
    }
  }
}
        â†“
Frontend: CallChecklist component
  - Mark item âœ…
  - Add "ğŸ“‹ Details" button
  - Modal shows evidence on click
```

### Caching strategy

```
Three levels of caching:

1. checklist_completion_cache: Dict[str, float]
   - Store timestamp of last check
   - Skip if checked < 30s ago

2. checklist_llm_cache: Dict[str, Dict]
   - Store LLM response for 60 seconds
   - Reuse for repeated checks

3. Permanent completion cache
   - Once item.completed = True
   - Never re-check (save LLM calls)
```

**Ğ›Ğ¾Ğ³Ğ¸ĞºĞ°:** permanent completion â†’ 30s check cooldown â†’ LLM semantic validation (0.8+ confidence) â†’ evidence extraction

---

## 5. ğŸ”„ ĞŸĞ¾Ğ»Ğ½Ñ‹Ğ¹ Ñ†Ğ¸ĞºĞ» Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Every 5 seconds (from buffer ready):                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Step 1: Transcription
  Audio Buffer (163KB) â†’ faster-whisper â†’ transcript string

Step 2: Speaker identification (if LLM enabled)
  transcript â†’ LLMAnalyzer.identify_speakers() â†’ client_text

Step 3: Client sentiment analysis
  client_text â†’ LLMAnalyzer.analyze_client_sentiment()
  â””â”€ Result: emotion, objections, interests, needs, engagement

Step 4: Trigger detection (In-Call Assist)
  transcript â†’ IntentDetector.detect_trigger()
  â””â”€ Match: keywords vs playbook
  â””â”€ Result: {id, title, hint, priority} or None
  â””â”€ Anti-spam: skip if same trigger active + < 30s

Step 5: Checklist validation
  For each uncompleted item:
    â”œâ”€ Skip if completed (permanent)
    â”œâ”€ Skip if checked < 30s ago
    â””â”€ LLMAnalyzer.check_checklist_item_semantic()
        â””â”€ Store evidence in checklist_evidence

Step 6: Stage detection
  transcript â†’ detect_stage_from_text()
  â””â”€ Result: greeting | profiling | presentation | objection | closing

Step 7: Broadcast via WebSocket
  Send JSON to all /coach clients:
  {
    "hint": "...",                        # from coach recommendation
    "prob": 0.8,                          # probability score
    "client_insight": {...},              # from LLM analysis
    "checklist_progress": {...},          # completion status
    "checklist_evidence": {...},          # text proof
    "current_stage": "objection",
    "next_step": "Address price objection...",
    "assist_trigger": {...} or null       # from IntentDetector
  }

Step 8: Frontend rate-limiting
  React hook: useEffect throttle (1 update/sec max)
  â””â”€ InCallAssist card updates
  â””â”€ ClientInfoSummary updates
  â””â”€ CallChecklist updates
```

---

## 6. ğŸ“Š Data Models

### CoachMessage (WebSocket /coach)

```typescript
interface CoachMessage {
  hint: string;                           // Sales coaching hint
  prob: number;                           // 0.0â€“1.0 probability
  client_insight: {
    emotion: string;
    objections: string[];
    interests: string[];
    needs: string | null;
    engagement_level: number;
    stage_hint: string;
    buying_signals: string[];
  };
  checklist_progress: Record<string, Record<string, {
    completed: boolean;
  }>>;
  checklist_evidence: Record<string, string>; // item_id â†’ evidence text
  current_stage: string;
  transcript_preview: string;
  next_step: string;
  assist_trigger?: {
    id: string;
    title: string;
    hint: string;
    priority: number;
  } | null;
}
```

### Global state (backend)

```python
# Audio & transcription
accumulated_transcript: str          # Full transcript so far
audio_buffer: AudioBuffer            # Current buffer instance
transcription_language: str          # 'en', 'id', 'ru', etc.
is_live_recording: bool              # True during live session

# Client insights (cached)
last_client_insight: Dict            # Latest analysis
last_hint: str                       # Last hint sent
last_prob: float                     # Last probability

# Checklist tracking
current_stage: str                   # Current call stage
checklist_progress: Dict             # item_id â†’ {completed: bool}
checklist_completion_cache: Dict     # item_id â†’ timestamp
checklist_llm_cache: Dict            # item_id â†’ {response, timestamp}
checklist_evidence: Dict             # item_id â†’ evidence text

# Intent detection
last_trigger_time: float             # Timestamp of last trigger
active_trigger_id: str | None        # Current active trigger ID
```

---

## 7. ğŸ› ï¸ Error handling

### Fallback strategy

```
If LLM fails:
  â””â”€ Use keyword-based analysis (client_insight.py)

If Whisper fails:
  â””â”€ Return empty transcript (no update sent)

If trigger detection fails:
  â””â”€ assist_trigger = None (no card shown)

If checklist LLM fails:
  â””â”€ Skip item, retry next cycle (cached for 60s)
```

### Guard clauses

```
analyze_client_sentiment:
  - Skip if text < 20 chars
  - Return neutral analysis

check_checklist_item_semantic:
  - Skip if text < 30 chars
  - Skip if LLM confidence < 0.8
  - Return False (don't mark complete)

detect_trigger:
  - Skip if text < 10 chars
  - Skip if same trigger + < 30s ago
  - Return None
```

---

## 8. ğŸ“ˆ Performance tuning

| Component | Update Interval | Cache | Cost |
|-----------|-----------------|-------|------|
| Transcription | 5s | None | 1 Whisper call/5s |
| Client Sentiment | 5s | None | 1 LLM call/5s |
| Trigger Detection | 5s | None | Regex only |
| Checklist Check | 5s | 30s cooldown + 60s LLM cache | 1 LLM call per item per 30s |
| WebSocket Rate | 1/sec | Throttle on frontend | Network only |

**Total cost per minute:**
- Whisper: 12 calls
- LLM (sentiment): 12 calls
- LLM (checklist): ~2â€“4 calls per item
- OpenRouter: ~$0.01â€“0.05/min (Claude 3 Haiku)
